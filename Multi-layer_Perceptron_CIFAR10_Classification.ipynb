{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPvy-9UPKU9l"
      },
      "source": [
        "3-layer MLP on the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvs_qpQCO2tu"
      },
      "source": [
        "Each input image is a 1D vector of size 3072 (=32x32x3).\n",
        "\n",
        "Implementing the following MLP model:\n",
        "\n",
        "```\n",
        "- Input: Bx3072 (B: batch size)\n",
        "- MLP\n",
        "  * Layer 1 (linear): #hidden units=512\n",
        "  * Nonlinearity: nn.ReLU\n",
        "  * Layer 2 (linear): #hidden units=128\n",
        "  * Nonlinearity: nn.ReLU  \n",
        "  * Layer 3 (linear): #hidden units=10 (10 classes)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "neis1BNxS2Ys"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# Input: Bx3072 (B: batch size)\n",
        "# - MLP\n",
        "  # * Layer 1 (linear): #hidden units=512\n",
        "  # * Nonlinearity: nn.ReLU\n",
        "  # * Layer 2 (linear): #hidden units=128\n",
        "  # * Nonlinearity: nn.ReLU  \n",
        "  # * Layer 3 (linear): #hidden units=10 (10 classes)\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "    #### your code starts ####\n",
        "    self.layer1 = nn.Linear(3072, 512)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.layer2 = nn.Linear(512, 128)\n",
        "    self.layer3 = nn.Linear(128, 10)\n",
        "    #### your code ends ####\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    #### your code starts ####        \n",
        "    x = self.layer1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.layer3(x)\n",
        "    #### your code ends ####\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H6k5wGbT1bD"
      },
      "source": [
        "Computing the cross-entropy loss between prediction and gt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24vMekycUfwO",
        "outputId": "29349214-5ff2-406d-cb87-204401539a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(606.3922)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# unit-test code\n",
        "batch_size = 5\n",
        "num_class = 10\n",
        "\n",
        "\n",
        "gt = torch.randint(0, num_class, (batch_size,))\n",
        "\n",
        "\n",
        "pred = torch.rand((batch_size, num_class)) * 1234\n",
        "\n",
        "\n",
        "# loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_unit_test = loss_fn(pred, gt)\n",
        "\n",
        "print(loss_unit_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWZB4n-FXDkg"
      },
      "source": [
        "Optimazation: SGD with `lr_rate = 0.01`.\n",
        "\n",
        "Creating a model from the MLP class\n",
        "Creating SGD optimizer upon this model's parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QivoQ0PaXDrb"
      },
      "outputs": [],
      "source": [
        "model = MLP()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhZBE0whNjet"
      },
      "source": [
        "Dataset: [CIFAR10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Each image is of the size 32x32x3, containing one main object out of 10 categories.\n",
        "\n",
        "<img height=400 src=\"http://bc-cv.github.io/csci3343/public/cifar10/cifar10.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXweDtDrJ48F"
      },
      "source": [
        "**Background:**\n",
        "\n",
        "For SGD, we need to create a dataset class and a dataloader class to randomly sample batches of data (image, label) to train the model. Following the example here [link](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXXva6f3OL3r"
      },
      "source": [
        "Downloading CIFAR10 data\n",
        "Creating Torch tensors `X_train` and `Y_train` using data_batch_1; `X_test` and `Y_test` using data_batch_2. Note that `X` is a Nx3072 matrix, where each image is flattened."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjgXwCSOHZo2",
        "outputId": "69ee77e8-765e-4042-b8aa-21a50fecde65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-10-17 21:58:05--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  27.7MB/s    in 6.1s    \n",
            "\n",
            "2022-10-17 21:58:13 (26.5 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "x cifar-10-batches-py/\n",
            "x cifar-10-batches-py/data_batch_4\n",
            "x cifar-10-batches-py/readme.html\n",
            "x cifar-10-batches-py/test_batch\n",
            "x cifar-10-batches-py/data_batch_3\n",
            "x cifar-10-batches-py/batches.meta\n",
            "x cifar-10-batches-py/data_batch_2\n",
            "x cifar-10-batches-py/data_batch_5\n",
            "x cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ],
      "source": [
        "! wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O cifar-10-python.tar.gz\n",
        "! tar -xzvf cifar-10-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e3xnRJalHZo4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# read in batch-1 as training data\n",
        "data_b1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
        "# each image is a 1D vector\n",
        "train_im1d = data_b1[b'data']\n",
        "train_im = train_im1d.reshape([train_im1d.shape[0],3,32,32]).transpose([0,2,3,1])\n",
        "train_label = data_b1[b'labels']\n",
        "\n",
        "# read in batch-2 as validation data\n",
        "data_b2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
        "val_im1d = data_b2[b'data']\n",
        "val_im = val_im1d.reshape([10000,3,32,32]).transpose([0,2,3,1])\n",
        "val_label = data_b2[b'labels']\n",
        "\n",
        "label_name = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cb-DstYGOMEm"
      },
      "outputs": [],
      "source": [
        "X_train = torch.from_numpy(train_im1d).float()\n",
        "Y_train = torch.from_numpy(np.array(train_label)).long()\n",
        "\n",
        "\n",
        "X_test = torch.from_numpy(val_im1d).float()\n",
        "Y_test = torch.from_numpy(np.array(val_label)).long()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxL9vb7DJZK8"
      },
      "source": [
        "  -  `__init__()`: makes attribute variables to record the input\n",
        "  -  `__len__()`: returns the number of images in the dataset\n",
        "  -  `__getitem__()`: Pytorch dataloader calls this function with an input random index `index` between `[0, len-1]` and ask for the return of `index`-th image and its label. The dataloader will later make them into the tensor format for each batch during each training iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0yfBlYS7MKxs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class CIFAR10Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, X, Y):\n",
        "    self.X = X/255\n",
        "    self.Y = Y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.Y[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nxw1Z5SmL1-7"
      },
      "outputs": [],
      "source": [
        "train_dataset = CIFAR10Dataset(X_train, Y_train)\n",
        "test_dataset = CIFAR10Dataset(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcMTrv4_Lf5B"
      },
      "source": [
        "- creating the `train_dataloader` and `test_dataloader` for each dataset with the following parameters:\n",
        "```\n",
        "batch_size: 64,\n",
        "shuffle: True,\n",
        "num_workers': 1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_vxFrS0LMK7d"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjY32gtieoVb"
      },
      "source": [
        "# <b>Train and test</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSeQu8mYPxUK"
      },
      "source": [
        "Train the model for 200 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0APSGAPfJ-Ki",
        "outputId": "f09c9e6b-37da-44fe-f2a3-981b334ec2c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Step [100/156], Loss: 2.2769\n",
            "Epoch [2/200], Step [100/156], Loss: 2.1854\n",
            "Epoch [3/200], Step [100/156], Loss: 1.9706\n",
            "Epoch [4/200], Step [100/156], Loss: 1.9789\n",
            "Epoch [5/200], Step [100/156], Loss: 1.9221\n",
            "Epoch [6/200], Step [100/156], Loss: 1.9348\n",
            "Epoch [7/200], Step [100/156], Loss: 1.9082\n",
            "Epoch [8/200], Step [100/156], Loss: 1.7156\n",
            "Epoch [9/200], Step [100/156], Loss: 1.8115\n",
            "Epoch [10/200], Step [100/156], Loss: 1.9201\n",
            "Epoch [11/200], Step [100/156], Loss: 1.8768\n",
            "Epoch [12/200], Step [100/156], Loss: 1.6641\n",
            "Epoch [13/200], Step [100/156], Loss: 1.7824\n",
            "Epoch [14/200], Step [100/156], Loss: 1.8132\n",
            "Epoch [15/200], Step [100/156], Loss: 1.6293\n",
            "Epoch [16/200], Step [100/156], Loss: 1.6594\n",
            "Epoch [17/200], Step [100/156], Loss: 1.6600\n",
            "Epoch [18/200], Step [100/156], Loss: 1.7549\n",
            "Epoch [19/200], Step [100/156], Loss: 1.6597\n",
            "Epoch [20/200], Step [100/156], Loss: 1.7039\n",
            "Epoch [21/200], Step [100/156], Loss: 1.6167\n",
            "Epoch [22/200], Step [100/156], Loss: 1.5836\n",
            "Epoch [23/200], Step [100/156], Loss: 1.6843\n",
            "Epoch [24/200], Step [100/156], Loss: 1.7503\n",
            "Epoch [25/200], Step [100/156], Loss: 2.0588\n",
            "Epoch [26/200], Step [100/156], Loss: 1.5756\n",
            "Epoch [27/200], Step [100/156], Loss: 1.6837\n",
            "Epoch [28/200], Step [100/156], Loss: 1.5855\n",
            "Epoch [29/200], Step [100/156], Loss: 1.5353\n",
            "Epoch [30/200], Step [100/156], Loss: 1.4690\n",
            "Epoch [31/200], Step [100/156], Loss: 1.5473\n",
            "Epoch [32/200], Step [100/156], Loss: 1.5955\n",
            "Epoch [33/200], Step [100/156], Loss: 1.6290\n",
            "Epoch [34/200], Step [100/156], Loss: 1.5595\n",
            "Epoch [35/200], Step [100/156], Loss: 1.4591\n",
            "Epoch [36/200], Step [100/156], Loss: 1.2064\n",
            "Epoch [37/200], Step [100/156], Loss: 1.3309\n",
            "Epoch [38/200], Step [100/156], Loss: 1.4292\n",
            "Epoch [39/200], Step [100/156], Loss: 1.5860\n",
            "Epoch [40/200], Step [100/156], Loss: 1.3364\n",
            "Epoch [41/200], Step [100/156], Loss: 1.5136\n",
            "Epoch [42/200], Step [100/156], Loss: 1.3748\n",
            "Epoch [43/200], Step [100/156], Loss: 1.5302\n",
            "Epoch [44/200], Step [100/156], Loss: 1.2631\n",
            "Epoch [45/200], Step [100/156], Loss: 1.4782\n",
            "Epoch [46/200], Step [100/156], Loss: 1.4010\n",
            "Epoch [47/200], Step [100/156], Loss: 1.6351\n",
            "Epoch [48/200], Step [100/156], Loss: 1.3395\n",
            "Epoch [49/200], Step [100/156], Loss: 1.2956\n",
            "Epoch [50/200], Step [100/156], Loss: 1.4458\n",
            "Epoch [51/200], Step [100/156], Loss: 1.2213\n",
            "Epoch [52/200], Step [100/156], Loss: 1.2410\n",
            "Epoch [53/200], Step [100/156], Loss: 1.3762\n",
            "Epoch [54/200], Step [100/156], Loss: 1.4750\n",
            "Epoch [55/200], Step [100/156], Loss: 1.5345\n",
            "Epoch [56/200], Step [100/156], Loss: 1.1253\n",
            "Epoch [57/200], Step [100/156], Loss: 1.2570\n",
            "Epoch [58/200], Step [100/156], Loss: 1.3025\n",
            "Epoch [59/200], Step [100/156], Loss: 1.2274\n",
            "Epoch [60/200], Step [100/156], Loss: 1.1427\n",
            "Epoch [61/200], Step [100/156], Loss: 1.1625\n",
            "Epoch [62/200], Step [100/156], Loss: 1.5056\n",
            "Epoch [63/200], Step [100/156], Loss: 1.1635\n",
            "Epoch [64/200], Step [100/156], Loss: 1.3423\n",
            "Epoch [65/200], Step [100/156], Loss: 1.2931\n",
            "Epoch [66/200], Step [100/156], Loss: 1.3899\n",
            "Epoch [67/200], Step [100/156], Loss: 1.1297\n",
            "Epoch [68/200], Step [100/156], Loss: 1.2395\n",
            "Epoch [69/200], Step [100/156], Loss: 1.2374\n",
            "Epoch [70/200], Step [100/156], Loss: 1.0870\n",
            "Epoch [71/200], Step [100/156], Loss: 1.3889\n",
            "Epoch [72/200], Step [100/156], Loss: 1.3488\n",
            "Epoch [73/200], Step [100/156], Loss: 1.2509\n",
            "Epoch [74/200], Step [100/156], Loss: 0.9822\n",
            "Epoch [75/200], Step [100/156], Loss: 1.2179\n",
            "Epoch [76/200], Step [100/156], Loss: 1.2039\n",
            "Epoch [77/200], Step [100/156], Loss: 0.9594\n",
            "Epoch [78/200], Step [100/156], Loss: 1.0657\n",
            "Epoch [79/200], Step [100/156], Loss: 1.3730\n",
            "Epoch [80/200], Step [100/156], Loss: 1.1832\n",
            "Epoch [81/200], Step [100/156], Loss: 1.0913\n",
            "Epoch [82/200], Step [100/156], Loss: 1.1232\n",
            "Epoch [83/200], Step [100/156], Loss: 1.1220\n",
            "Epoch [84/200], Step [100/156], Loss: 0.9238\n",
            "Epoch [85/200], Step [100/156], Loss: 0.8604\n",
            "Epoch [86/200], Step [100/156], Loss: 0.9109\n",
            "Epoch [87/200], Step [100/156], Loss: 1.0325\n",
            "Epoch [88/200], Step [100/156], Loss: 1.1781\n",
            "Epoch [89/200], Step [100/156], Loss: 1.0301\n",
            "Epoch [90/200], Step [100/156], Loss: 1.4040\n",
            "Epoch [91/200], Step [100/156], Loss: 0.9204\n",
            "Epoch [92/200], Step [100/156], Loss: 1.1609\n",
            "Epoch [93/200], Step [100/156], Loss: 1.3874\n",
            "Epoch [94/200], Step [100/156], Loss: 0.9061\n",
            "Epoch [95/200], Step [100/156], Loss: 1.0829\n",
            "Epoch [96/200], Step [100/156], Loss: 0.8482\n",
            "Epoch [97/200], Step [100/156], Loss: 1.2913\n",
            "Epoch [98/200], Step [100/156], Loss: 0.9524\n",
            "Epoch [99/200], Step [100/156], Loss: 0.9626\n",
            "Epoch [100/200], Step [100/156], Loss: 1.0076\n",
            "Epoch [101/200], Step [100/156], Loss: 0.8641\n",
            "Epoch [102/200], Step [100/156], Loss: 0.7868\n",
            "Epoch [103/200], Step [100/156], Loss: 0.9477\n",
            "Epoch [104/200], Step [100/156], Loss: 1.3012\n",
            "Epoch [105/200], Step [100/156], Loss: 0.8813\n",
            "Epoch [106/200], Step [100/156], Loss: 0.9358\n",
            "Epoch [107/200], Step [100/156], Loss: 0.9128\n",
            "Epoch [108/200], Step [100/156], Loss: 0.7767\n",
            "Epoch [109/200], Step [100/156], Loss: 1.1073\n",
            "Epoch [110/200], Step [100/156], Loss: 0.8690\n",
            "Epoch [111/200], Step [100/156], Loss: 1.0955\n",
            "Epoch [112/200], Step [100/156], Loss: 0.9152\n",
            "Epoch [113/200], Step [100/156], Loss: 0.7272\n",
            "Epoch [114/200], Step [100/156], Loss: 0.8877\n",
            "Epoch [115/200], Step [100/156], Loss: 0.8422\n",
            "Epoch [116/200], Step [100/156], Loss: 0.8637\n",
            "Epoch [117/200], Step [100/156], Loss: 1.0282\n",
            "Epoch [118/200], Step [100/156], Loss: 0.7407\n",
            "Epoch [119/200], Step [100/156], Loss: 0.7554\n",
            "Epoch [120/200], Step [100/156], Loss: 0.6296\n",
            "Epoch [121/200], Step [100/156], Loss: 0.7215\n",
            "Epoch [122/200], Step [100/156], Loss: 0.7883\n",
            "Epoch [123/200], Step [100/156], Loss: 0.6647\n",
            "Epoch [124/200], Step [100/156], Loss: 0.8387\n",
            "Epoch [125/200], Step [100/156], Loss: 0.8563\n",
            "Epoch [126/200], Step [100/156], Loss: 0.7984\n",
            "Epoch [127/200], Step [100/156], Loss: 0.5880\n",
            "Epoch [128/200], Step [100/156], Loss: 0.7817\n",
            "Epoch [129/200], Step [100/156], Loss: 0.7058\n",
            "Epoch [130/200], Step [100/156], Loss: 0.7326\n",
            "Epoch [131/200], Step [100/156], Loss: 0.8430\n",
            "Epoch [132/200], Step [100/156], Loss: 0.6597\n",
            "Epoch [133/200], Step [100/156], Loss: 0.6552\n",
            "Epoch [134/200], Step [100/156], Loss: 0.8045\n",
            "Epoch [135/200], Step [100/156], Loss: 0.5190\n",
            "Epoch [136/200], Step [100/156], Loss: 0.5977\n",
            "Epoch [137/200], Step [100/156], Loss: 0.8140\n",
            "Epoch [138/200], Step [100/156], Loss: 0.7467\n",
            "Epoch [139/200], Step [100/156], Loss: 0.5838\n",
            "Epoch [140/200], Step [100/156], Loss: 0.7515\n",
            "Epoch [141/200], Step [100/156], Loss: 0.6480\n",
            "Epoch [142/200], Step [100/156], Loss: 0.6485\n",
            "Epoch [143/200], Step [100/156], Loss: 0.5787\n",
            "Epoch [144/200], Step [100/156], Loss: 0.6189\n",
            "Epoch [145/200], Step [100/156], Loss: 0.6588\n",
            "Epoch [146/200], Step [100/156], Loss: 0.5032\n",
            "Epoch [147/200], Step [100/156], Loss: 0.4781\n",
            "Epoch [148/200], Step [100/156], Loss: 1.0836\n",
            "Epoch [149/200], Step [100/156], Loss: 0.7191\n",
            "Epoch [150/200], Step [100/156], Loss: 0.5816\n",
            "Epoch [151/200], Step [100/156], Loss: 0.4104\n",
            "Epoch [152/200], Step [100/156], Loss: 0.8834\n",
            "Epoch [153/200], Step [100/156], Loss: 0.5810\n",
            "Epoch [154/200], Step [100/156], Loss: 0.5569\n",
            "Epoch [155/200], Step [100/156], Loss: 0.7448\n",
            "Epoch [156/200], Step [100/156], Loss: 0.5628\n",
            "Epoch [157/200], Step [100/156], Loss: 0.4815\n",
            "Epoch [158/200], Step [100/156], Loss: 0.4114\n",
            "Epoch [159/200], Step [100/156], Loss: 0.3281\n",
            "Epoch [160/200], Step [100/156], Loss: 0.3930\n",
            "Epoch [161/200], Step [100/156], Loss: 0.5227\n",
            "Epoch [162/200], Step [100/156], Loss: 0.4439\n",
            "Epoch [163/200], Step [100/156], Loss: 0.5289\n",
            "Epoch [164/200], Step [100/156], Loss: 0.3783\n",
            "Epoch [165/200], Step [100/156], Loss: 0.3818\n",
            "Epoch [166/200], Step [100/156], Loss: 0.4303\n",
            "Epoch [167/200], Step [100/156], Loss: 0.2723\n",
            "Epoch [168/200], Step [100/156], Loss: 0.5185\n",
            "Epoch [169/200], Step [100/156], Loss: 0.2699\n",
            "Epoch [170/200], Step [100/156], Loss: 0.3343\n",
            "Epoch [171/200], Step [100/156], Loss: 0.2793\n",
            "Epoch [172/200], Step [100/156], Loss: 0.4282\n",
            "Epoch [173/200], Step [100/156], Loss: 0.7709\n",
            "Epoch [174/200], Step [100/156], Loss: 0.4205\n",
            "Epoch [175/200], Step [100/156], Loss: 0.3731\n",
            "Epoch [176/200], Step [100/156], Loss: 0.2257\n",
            "Epoch [177/200], Step [100/156], Loss: 0.2221\n",
            "Epoch [178/200], Step [100/156], Loss: 0.3030\n",
            "Epoch [179/200], Step [100/156], Loss: 0.4827\n",
            "Epoch [180/200], Step [100/156], Loss: 0.2396\n",
            "Epoch [181/200], Step [100/156], Loss: 0.4354\n",
            "Epoch [182/200], Step [100/156], Loss: 0.3311\n",
            "Epoch [183/200], Step [100/156], Loss: 0.6450\n",
            "Epoch [184/200], Step [100/156], Loss: 0.4667\n",
            "Epoch [185/200], Step [100/156], Loss: 0.5292\n",
            "Epoch [186/200], Step [100/156], Loss: 0.3302\n",
            "Epoch [187/200], Step [100/156], Loss: 0.2662\n",
            "Epoch [188/200], Step [100/156], Loss: 0.2664\n",
            "Epoch [189/200], Step [100/156], Loss: 0.2771\n",
            "Epoch [190/200], Step [100/156], Loss: 0.2173\n",
            "Epoch [191/200], Step [100/156], Loss: 0.4272\n",
            "Epoch [192/200], Step [100/156], Loss: 0.8877\n",
            "Epoch [193/200], Step [100/156], Loss: 0.4754\n",
            "Epoch [194/200], Step [100/156], Loss: 0.5268\n",
            "Epoch [195/200], Step [100/156], Loss: 0.1885\n",
            "Epoch [196/200], Step [100/156], Loss: 0.2194\n",
            "Epoch [197/200], Step [100/156], Loss: 0.2871\n",
            "Epoch [198/200], Step [100/156], Loss: 2.5775\n",
            "Epoch [199/200], Step [100/156], Loss: 0.4280\n",
            "Epoch [200/200], Step [100/156], Loss: 0.2812\n"
          ]
        }
      ],
      "source": [
        "# put model on GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# training loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, len(train_dataset)//64, loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL9j-q56ouUB",
        "outputId": "14770965-645e-4dd1-9426-3d61923ccb8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 41.27000045776367 %\n"
          ]
        }
      ],
      "source": [
        "# testing loop\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        correct += (labels.data == outputs.argmax(axis=1)).sum()\n",
        "        total += labels.size(0)\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(accuracy))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
